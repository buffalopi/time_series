
# coding: utf-8

# ## 中文分词简述:
# 中文文本词与词之间不像英文一样有空格,所以涉及到的文本操作要分词.
# 
# 
# 分词的难点在于**`消除歧义`**, **`词的颗粒度选择问题`**，分词歧义主要包括如下几个方面：
# <br>
# 交集歧义, 例如：
# 
# 研究/ 生命/ 的/ 起源
# <br>
# 研究生/ 命/ 的/ 起源
# <br>
# 组合歧义，例如：
# <br>
# 他 / 从 / 马 / 上 / 下来
# <br>
# 他 / 从 / 马上 / 下来
# <br>
# 未登录词，例如：
# <br>
# 蔡英文 / 和 / 特朗普 / 通话
# <br>
# 蔡英文 / 和 / 特朗 / 普通话
# <br>
# 上下文歧义,例如:
# <br>
# 乒乓/球拍/卖/完/了
# <br>
# 乒乓球/拍卖/完/了
# <br>
# 词的颗粒度选择问题是分词的一个难题。研究者们往往把"结合紧密、使用稳定"视为分词单位的界定准则，
# <br>
# 然而人们对于这种准则理解的主观性差别较大，受到个人的知识结构和所处环境的很大影响

# In[ ]:

中文分词工具:
中文分词工具多如牛毛,中科院的ICTCLAS，哈工大的ltp，东北大学的NIU Parser是学术界著名的分词器,其他比较出名的还有jieba,盘古分词,Stanford CoreNLP


# In[ ]:

中文分词原理:
现有的方法分为三大类:基于字符串匹配(规则),基于理解(语义)的分词,基于统计的分词


# In[ ]:

基于规则的策略,按照一定的策略将待分析的词条与字典库匹配,该方法有三个要素,分词词典,文本扫描顺序和匹配原则.
实现简单，效果尚可；但对歧义和未登录词处理效果不佳.

南京市长江大桥
南京,市长,长江,大桥 这些常用词易出现在字典库,所以顺序分词成 南京|市长|江|大桥,逆序分词为南京市|长江|大桥


# In[ ]:

基于理解的分词方法,是通过计算机模拟人对句子的理解,达到识别词的效果.基本思想是在分词的时候同时进行句法,语义分析,利用句法信息和
语义信息处理歧义现象.这种分词方法需要使用大量的语言知识和信息,由于汉语语言知识的笼统,复杂性,将难以将各种语言组织成机器可直接读取的形式,
因此尚处在试验阶段


# In[ ]:

基于统计的分词方法,是在给定大量已经分词的文本前提下,利用统计机器学习模型学习词语切分规律,从而实现对未知文本的切分.
例如最大概率分词方法和最大熵分词方法.随着大规模语料库的建立,基于统计的中文分词方法渐渐成为主流方法.
主要的统计模型有：N元文法模型（N-gram），隐马尔可夫模型（Hidden Markov Model ，HMM），
最大熵模型（ME），条件随机场模型（Conditional Random Fields，CRF）等

在实际的应用中，基于统计的分词系统都需要使用分词词典来进行字符串匹配分词，同时使用统计方法识别一些新词.
依赖于一个事先编制的词表("(词典)，自动分词过程就是通过词表和相关信息来做出词语切分的决策")


# In[ ]:

n-gram:第n个词的出现只与前面n-1个词相关,常用的场景输入法,搜索时的猜词


# In[ ]:

新的改进:基于字标注（或者叫基于序列标注）的分词方法实际上是构词方法，即把分词过程视为字在字串中的标注问题.
一个重要优势在于，它能够平衡地看待词表词和未登录词的识别问题
假如规定每个字最多只有四个构词位置：即B(词首)，M (词中)，E(词尾)和S(单独成词)，那么下面句子(甲)的分词结果就可以直接表示成如(乙)所示的逐字标注形式：
(甲)分词结果：／上海／计划／N／本／世纪／末／实现／人均／国内／生产／总值／五千美元／ 
(乙)字标注形式：上／B海／E计／B划／E N／S 本／s世／B 纪／E 末／S 实／B 现／E 人／B 均／E 国／B 内／E生／B产／E总／B值／E 五／B千／M 美／M 元／E 。／S


# In[ ]:

常用python中文分词库
jieba是国内使用人数最多的中文分词工具:
jieba分词支持三种模式：

（1）精确模式：试图将句子最精确地切开，适合文本分析；

（2）全模式：把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义；

（3）搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。

jieba分词过程中主要涉及如下几种算法：

（1）基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG)；

（2）采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合；

（3）对于未登录词，采用了基于汉字成词能力的 HMM 模型，采用Viterbi 算法进行计算；

（4）基于Viterbi算法做词性标注；

（5）基于tf-idf和textrank模型抽取关键词；


# In[2]:


import jieba
#全模式
test1 = jieba.cut("杭州西湖风景很好，是旅游胜地！", cut_all=True)
print("全模式: " + "| ".join(test1))


# In[7]:

#精确模式
test2 = jieba.cut("杭州西湖风景很好，是旅游胜地！", cut_all=False)
print("精确模式: " + "| ".join(test2))


# In[8]:

#搜索引擎模式
test3= jieba.cut_for_search("杭州西湖风景很好，是旅游胜地,每年吸引大量前来游玩的游客！")  
print("搜索引擎模式:" + "| ".join(test3))


# In[ ]:

SnowNLP是一个python写的类库，可以方便的处理中文文本内容，是受到了TextBlob的启发而写的。SnowNLP主要包括如下几个功能：

（1）中文分词（Character-Based Generative Model）；

（2）词性标注（3-gram HMM）；

（3）情感分析（简单分析，如评价信息）；

（4）文本分类（Naive Bayes）

（5）转换成拼音（Trie树实现的最大匹配）

（6）繁简转换（Trie树实现的最大匹配）

（7）文本关键词和文本摘要提取（TextRank算法）

（8）计算文档词频（TF，Term Frequency）和逆向文档频率（IDF，Inverse Document Frequency）

（9）Tokenization（分割成句子）

（10）文本相似度计算（BM25）

SnowNLP的最大特点是特别容易上手，用其处理中文文本时能够得到不少有意思的结果，但不少功能比较简单，还有待进一步完善。


# In[10]:


from snownlp import SnowNLP
s=SnowNLP(u'杭州西湖风景很好，是旅游胜地,每年吸引大量前来游玩的游客！')
#分词
print(s.words)


# In[11]:

#情感词性计算
print("该文本的情感词性为正的概率:" + str(s.sentiments))


# In[12]:

_s=SnowNLP(u'今天又是下雨又是刮风,真是糟糕透了!')
print("该文本的情感词性为正的概率:" + str(_s.sentiments))


# In[13]:

text = u'''
西湖，位于浙江省杭州市西面，是中国大陆首批国家重点风景名胜区和中国十大风景名胜之一。
它是中国大陆主要的观赏性淡水湖泊之一，也是现今《世界遗产名录》中少数几个和中国唯一一个湖泊类文化遗产。
西湖三面环山，面积约6.39平方千米，东西宽约2.8千米，南北长约3.2千米，绕湖一周近15千米。
湖中被孤山、白堤、苏堤、杨公堤分隔，按面积大小分别为外西湖、西里湖、北里湖、小南湖及岳湖等五片水面，
苏堤、白堤越过湖面，小瀛洲、湖心亭、阮公墩三个小岛鼎立于外西湖湖心，夕照山的雷峰塔与宝石山的保俶塔隔湖相映，
由此形成了“一山、二塔、三岛、三堤、五湖”的基本格局。
'''

s2 = SnowNLP(text)

#文本关键词提取
print(s2.keywords(10))


# In[ ]:

THULAC（THU Lexical Analyzer for Chinese）由清华大学自然语言处理与社会人文计算实验室研制推出的一套中文词法分析工具包，
具有中文分词和词性标注功能。THULAC具有如下几个特点：

（1）能力强。利用我们集成的目前世界上规模最大的人工分词和词性标注中文语料库（约含5800万字）训练而成，模型标注能力强大。

（2）准确率高。该工具包在标准数据集Chinese Treebank（CTB5）上分词的F1值可达97.3％，词性标注的F1值可达到92.9％，与该数据集上最好方法效果相当。

（3）速度较快。同时进行分词和词性标注速度为300KB/s，每秒可处理约15万字。只进行分词速度可达到1.3MB/s。


# In[3]:

import thulac   

#默认模式，分词的同时进行词性标注
test1 = thulac.thulac()
text1 = test1.cut("杭州西湖风景很好，是旅游胜地！")
print(text1)


#只进行分词
test2 = thulac.thulac(seg_only=True)
text2 = test2.cut("杭州西湖风景很好，是旅游胜地！")
print(text2)


# In[ ]:



